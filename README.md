# Metadata Extractor

Built a FastAPI-based NLP pipeline using spaCy to extract dates, authors, and key terms from unstructured PDF, DOCX, and TXT files via rule-based and NER methods.
Dockerized and deployed the service with scalable APIs, adding logging, validation, and testing for robust metadata extraction in research documents.
=======
H
```markdown
# ðŸ§  Metadata Extraction API

A production-ready FastAPI-based service that extracts structured metadata (dates, authors, and key terms) from uploaded documents using a combination of **rule-based** and **statistical NLP techniques** (via spaCy). It supports multiple file formats and provides RESTful endpoints for ease of integration.

---

## ðŸš€ Features

- ðŸ” **Extracts metadata**: dates, authors, and domain-specific key terms  
- ðŸ“„ **Supports**: `.pdf`, `.docx`, `.txt`, `.md`  
- ðŸ§  **spaCy-powered NLP** pipeline:  
  - Rule-based matcher for dates and key terms  
  - Named Entity Recognition (NER) for authors, dates, orgs, locations  
- âš™ï¸ REST API built with **FastAPI**  
- ðŸ§ª Full **testing suite** using PyTest  
- ðŸ³ **Dockerized** for easy deployment  
- ðŸ§¾ JSON output and structured logs  

---

## ðŸ—‚ï¸ Project Structure

```

metadata-extraction/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # FastAPI application and endpoints
â”‚   â”œâ”€â”€ models.py              # Pydantic models for input/output
â”‚   â”œâ”€â”€ extraction/            # Core logic for metadata extraction
â”‚   â”‚   â”œâ”€â”€ patterns.py
â”‚   â”‚   â”œâ”€â”€ metadata\_extractor.py
â”‚   â”‚   â””â”€â”€ text\_extractor.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ file\_handlers.py   # File validation and output writing
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample\_documents/      # Sample files for demo
â”‚   â””â”€â”€ output/                # JSON output directory
â”œâ”€â”€ logs/                      # App logs
â”œâ”€â”€ tests/                     # Pytest unit and API tests
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md

````

---

## âš™ï¸ Setup & Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/metadata-extraction.git
cd metadata-extraction
````

### 2. Install Dependencies

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

---

## ðŸ§ª Run Locally

### Start the API

```bash
uvicorn app.main:app --reload
```

### API Access

* Base URL: `http://localhost:8000`
* Interactive Docs: `http://localhost:8000/docs`

---

## ðŸ” API Endpoints

| Method | Endpoint          | Description           |
| ------ | ----------------- | --------------------- |
| GET    | `/`               | Health check message  |
| GET    | `/health`         | Full health status    |
| POST   | `/extract`        | Upload multiple files |
| POST   | `/extract-single` | Upload a single file  |

---

## ðŸ“¥ Example Request

### Upload a single file via Python

```python
import requests

with open("sample_research.txt", "rb") as f:
    files = {"file": ("sample_research.txt", f)}
    r = requests.post("http://localhost:8000/extract-single", files=files)
    print(r.json())
```

---

## ðŸ§ª Run Tests

```bash
pytest tests/
```

---

## ðŸ³ Docker Deployment

### Build & Run

```bash
docker-compose up --build
```

Then open `http://localhost:8000/docs` to test your endpoints.

---

## âœ… Sample Output (JSON)

```json
{
  "file_name": "sample_research.txt",
  "dates": ["March 15, 2024", "January 2024", "February 2024"],
  "authors": ["Dr. John Smith", "Prof. Sarah Johnson"],
  "key_terms": ["machine learning", "natural language processing", "neural network"]
}
```

---

## ðŸ“Œ Notes

* File size limit: 10 MB
* Only text-based files are supported (PDFs must have extractable text)
* Add more patterns in `patterns.py` to suit specific use cases/domains

---

## ðŸ‘¨â€ðŸ’» Author

Built with â¤ï¸ by \[Siddhartha Sahu]
GitHub: \[SiSuSa]
Email: [sidbeingreal@gmail.com](mailto:sidbeingreal@gmail.com)

---

## ðŸ“œ License

MIT License


>>>>>>> ee3d307 (Initial commit: Add complete metadata extraction project)
